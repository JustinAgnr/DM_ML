{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Graded.\n",
    "\n",
    "**Due date december 1st**\n",
    "You can also send your answers to the optional questions \n",
    "\n",
    "1. Based on the minimal code example with validation and $L_2$ prenality, implement your own version of a factorization machine. As we do want to use fields this is the same than a rank k asumption on the matrix of the second order terms which can be expressed as $<v_i, v_j>$ with $v_i \\in \\mathbb{R}^k$ .  Remind that \n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta} \\hat{y}(\\mathbf{x})= \\begin{cases}1, & \\text { if } \\theta \\text { is } \\mbox{the bias term } \\omega_0 \\\\ x_{i}, & \\text { if } \\theta \\text { is } w_{i} \\mbox{ (first order term)} \\\\ x_{i} \\sum_{j=1}^{n} v_{j, f} x_{j}-v_{i, f} x_{i}^{2}, & \\text { if } \\theta \\text { is the } i,f \\mbox { second order term } <v_i,v_f>   \\end{cases}\n",
    "$$\n",
    "\n",
    "One possibility to code it is to isolate the second order term within a second vector of parameters while performing the ```get_x``` and change the way the gradient is computed in the ```update``` function. Details about factorization machines are in the [original paper](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-01-14 14:33:43--  http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip\n",
      "Resolving go.criteo.net (go.criteo.net)... 178.250.0.152\n",
      "Connecting to go.criteo.net (go.criteo.net)|178.250.0.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://criteostorage.blob.core.windows.net/criteo-research-datasets/criteo_ppml_challenge_adkdd2021/criteo-ppml-challenge-adkdd21-dataset.zip [following]\n",
      "--2022-01-14 14:33:44--  https://criteostorage.blob.core.windows.net/criteo-research-datasets/criteo_ppml_challenge_adkdd2021/criteo-ppml-challenge-adkdd21-dataset.zip\n",
      "Resolving criteostorage.blob.core.windows.net (criteostorage.blob.core.windows.net)... 20.209.1.1\n",
      "Connecting to criteostorage.blob.core.windows.net (criteostorage.blob.core.windows.net)|20.209.1.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 297834690 (284M) [application/zip]\n",
      "Saving to: ‘criteo-ppml-challenge-adkdd21-dataset.zip’\n",
      "\n",
      "l-challenge-adkdd21   4%[                    ]  13.34M   143KB/s    eta 7m 24s "
     ]
    }
   ],
   "source": [
    "!wget http://go.criteo.net/criteo-ppml-challenge-adkdd21-dataset.zip\n",
    "!unzip criteo-ppml-challenge-adkdd21-dataset.zip\n",
    "!rm aggregated_noisy_data_pairs.csv.gz aggregated_noisy_data_singles.csv.gz  \n",
    "!gunzip X_test.csv.gz X_train.csv.gz y_test.csv.gz y_train.csv.gz\n",
    "!rm criteo-ppml-challenge-adkdd21-dataset.zip\n",
    "!sed -n 1,10000p X_test.csv > X_valid.csv #CARE we took 10k lines from test as validation.\n",
    "!sed -n 1,10000p y_test.csv > y_valid.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer question 1 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer that question we introduce the matrix ```v``` that has a ```(D,K)``` shape. After that we modified the ```get_p``` and ```update_w``` functions : \n",
    "- ```get_p_factorization``` know that into account the $\\sum_{i=1}^{n}\\sum_{j=i+1}^{n} x_i x_j v_{i, j}$ part.\n",
    "- ```update_wL2_factorization``` update the values for the matrix ```v``` with an L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - same stuff with a L2 regularizer (bias term + quad terms)\n",
    "# A lot of settings allows a large difference between train and validation, theses are indeed bad\n",
    "# Remark than combining L1 and L2 is easy and is often named \"elasticnet\"\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "\n",
    "# ANSWER with ony one vector for parameters. This is also possible to split the first and second order terms\n",
    "def get_x(csv_row, D):\n",
    "    x = []\n",
    "    # normal features\n",
    "    for key, value in csv_row.items():\n",
    "        index = int(value + key[4:], 16) % D\n",
    "        x.append(index)\n",
    "    x.append(0)\n",
    "    return x\n",
    "\n",
    "\n",
    "def compute_validation_loss_factorization(w, D, v, k):\n",
    "    ### TODO Complete this function ###\n",
    "    # ANSWER\n",
    "    val_loss = 0\n",
    "    for t, (row, y) in enumerate(zip(DictReader(open(X_valid)), DictReader(open(y_valid)))):\n",
    "        x = get_x(row, D)\n",
    "        p = get_p_factorization(x, w, v, k)\n",
    "        target = float(y['click'])\n",
    "        val_loss += logloss(p, target)\n",
    "    return val_loss / t\n",
    "\n",
    "\n",
    "def get_p_factorization(x, w, v, k):\n",
    "    wTx = 0.\n",
    "    for i in x:\n",
    "        # wTx\n",
    "        wTx += w[i] * 1.  # w[i] * 1\n",
    "\n",
    "        # <v_i, v_j>*(x_i * x_j)\n",
    "\n",
    "    for f in range(k):\n",
    "        sv = np.sum(v[x, f] * 1) ** 2\n",
    "        wTx += 0.5 * (sv - np.sum(v[x, f] ** 2 * 1))\n",
    "\n",
    "    return 1. / (1. + exp(-max(min(wTx, 20.), -20.)))  # bounded sigmoid\n",
    "\n",
    "\n",
    "def update_wL2_factorization(w, v, n, x, p, y, k, lbd=5e-4, alpha=.01):\n",
    "    for index, value in enumerate(x):\n",
    "        # Update w \n",
    "        w[value] -= ((p - y) + lbd * 2 * w[value]) * alpha / (sqrt(n[value]) + 1.)\n",
    "        n[value] += 1.\n",
    "    # Update v with L2 regularization\n",
    "    for f in range(k):\n",
    "        sv = np.sum(v[x, f])\n",
    "        for index, value in enumerate(x):\n",
    "            v[value, f] -= ((p - y) * (sv - v[value, f]) + lbd * 2 * v[value, f]) * alpha\n",
    "\n",
    "    return w, v, n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DictReader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f1050c09d431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mtraining_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDictReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# I gather the memory placement of the coefficients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_p_factorization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# I gather the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DictReader' is not defined"
     ]
    }
   ],
   "source": [
    "D = 2 ** 24\n",
    "w = [0.] * D  \n",
    "n = [0.] * D\n",
    "loss = 0.\n",
    "n_epochs = 1\n",
    "n_updates = 0\n",
    "training_losses = [] \n",
    "validation_losses = [] \n",
    "alpha = .01 \n",
    "k=5\n",
    "\n",
    "# Initialisation de v\n",
    "import numpy as np\n",
    "v = np.full(shape = (D, k), fill_value = -0.01)\n",
    "\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "  training_loss = 0\n",
    "  for t, (row, y)  in enumerate(zip(DictReader(open(X_opt)), DictReader(open(y_opt)))):\n",
    "      x = get_x(row, D)  # I gather the memory placement of the coefficients\n",
    "      p = get_p_factorization(x, w, v, k) # I gather the prediction\n",
    "      target = float(y['click'])\n",
    "      training_loss += logloss(p, target) # I compare my model\n",
    "      if n_updates% 10000 == 0 and n_updates>1:\n",
    "          training_losses.append( training_loss/t )\n",
    "          validation_losses.append( compute_validation_loss_factorization(w, D, v, k) )\n",
    "          print('%s\\tupdates: %d\\tcurrent logloss on train: %f\\tcurrent logloss on validation: %f' % (\n",
    "              datetime.now(), n_updates, training_losses[-1], validation_losses[-1] ))\n",
    "      w, v, n = update_wL2_factorization(w, v, n, x, p, target, k) # Upatede the coefficients  \n",
    "      n_updates += 1\n",
    "\n",
    "x = [10000*i for i in range(len(training_losses))]\n",
    "plt.plot(x, training_losses, label='Train')\n",
    "plt.plot(x, validation_losses, label='Validation')\n",
    "plt.xlabel('Number of updates')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.legend( ('Train', 'Validation') )\n",
    "plt.show() \n",
    "\n",
    "wL2 = w #keep it for later use\n",
    "\n",
    "#Depending on the learning rate and the coefficent in front of regularization this is possible \n",
    "#to have some overfitting advocating for early stopping. Remark that it does not hurt as hard the validation performance \n",
    "#Gains remains quite unclear, possibly due to the small dataset size   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
