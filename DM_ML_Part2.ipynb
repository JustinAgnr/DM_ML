{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Advanced Machine Learning : Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used **onyxia** virtual machines to speed up the calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install ftfy regex tqdm\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:23<00:00, 15.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import clip\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=clip_preprocess)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=clip_preprocess)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a first net that uses the clip model as a backbone. To do so, we use the ```encode_image``` method after loading the clip model with ```import clip```. As we found out the ```encode_image``` gives an tensor of dimension 512. We than apply to linear transformations to get the output size (=10). We first apply the ```gelu``` activation function as suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(512, 84)\n",
    "        self.fc2 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = model.encode_image(x).float()\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an instance of the net : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then to use a **stochastic gradient descent** and an **entropy loss function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=1e-4, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define arrays to keep track of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_in_time = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the usual loop for training the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [02:26<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: train loss 2.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 782/782 [05:11<00:00,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: train loss 2.1154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(tqdm(trainloader, 0)):\n",
    "        # load inputs to the GPU\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients this is important to do it at each batch since the gradient are accumulated\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward - Graph is created\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # We compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss_in_time.append(loss.item())\n",
    "\n",
    "        # backward - gradients of the the parameters of the model are computed\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient step on the parameters referenced in the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item() * len(labels)\n",
    "\n",
    "    # print every epoch\n",
    "    print(f'Epoch {epoch:03}: train loss {running_loss / len(trainset):0.5}')\n",
    "    running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5f9edd3cf8>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(loss_in_time[i for i in range(len)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we **test** the model : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:25<00:00,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 78.1%, Avg loss: 4.950397 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, correct = 0, 0\n",
    "size = len(testloader.dataset)\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(tqdm(testloader, 0)):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "\n",
    "test_loss /= 64\n",
    "correct /= size\n",
    "print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the hymenoptera file from the this link ```https://download.pytorch.org/tutorial/hymenoptera_data.zip```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could not download file bigger than 15 Mb, we use the request librairy to do so : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47286322"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "open('hymenoptera_data.zip', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then unzip the result : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile('hymenoptera_data.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the **train** and **val** datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {x: torchvision.datasets.ImageFolder(os.path.join(os.getcwd(), 'hymenoptera_data', x), clip_preprocess) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the correspondind dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x: torch.utils.data.DataLoader(datasets[x], batch_size=4, shuffle=True, num_workers=4) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(image[x]) for x in ['train', 'val']} #{'train': 244, 'val': 153}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataloaders['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f5f5ea766a0>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-202-dd9a1c746732>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare the inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# image, class_id = cifar100[3637]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimage_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"a photo of a {c}\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# Prepare the inputs\n",
    "# image, class_id = cifar100[3637]\n",
    "image_input = inputs.to(device)\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_input) == len(text_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the clip model to get the enconding of both image and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # We do not need to update the weigths\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-23b3625ba651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTop predictions:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{image.classes[index]:>16s}: {100 * value.item():.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'classes'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{image.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F5F5EA76F98>\n",
      "1\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset\n",
    "cifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n",
    "# C'est un torchvision.dataset de taille 10000\n",
    "# Prepare the inputs\n",
    "image, class_id = cifar100[3636]\n",
    "print(image)\n",
    "image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "print(len(image_input))\n",
    "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
    "print(len(text_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate features\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Pick the top 5 most similar labels for the image\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
